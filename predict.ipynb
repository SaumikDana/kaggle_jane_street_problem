{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from methods import create_timeseries_for_symbol, prepare_regression_data, tune_xgboost, evaluate_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def main():\n",
    "\n",
    "    # Load and prepare data (with all features and responders)\n",
    "    df = pd.read_parquet(\"train.parquet/partition_id=0/part-0.parquet\")\n",
    "    list_of_symbols = df['symbol_id'].unique()\n",
    "\n",
    "    print(f\"\\n Total number of symbols (financial instruments) is {len(list_of_symbols)}\")\n",
    "    symbol_id = list_of_symbols[0]\n",
    "\n",
    "    for symbol_id in list_of_symbols:\n",
    "        print(f\"\\nTraining and Evaluating for symbol {symbol_id} ...\")\n",
    "        features, responders, target = create_timeseries_for_symbol(df, symbol_id)\n",
    "        X, y = prepare_regression_data(features, responders, target)\n",
    "        \n",
    "        # Tune model\n",
    "        best_model = tune_xgboost(X, y)\n",
    "        \n",
    "        # Evaluate best model\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        evaluate_model(best_model, X_train, X_test, y_train, y_test)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import train_best_xgboost_model, evaluate_best_model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and prepare data (with all features and responders)\n",
    "    df = pd.read_parquet(\"train.parquet/partition_id=0/part-0.parquet\")\n",
    "    list_of_symbols = df['symbol_id'].unique()\n",
    "\n",
    "    print(f\"\\n Total number of symbols (financial instruments) is {len(list_of_symbols)}\")\n",
    "    symbol_id = list_of_symbols[0]\n",
    "\n",
    "    for symbol_id in list_of_symbols:\n",
    "        print(f\"\\nTraining and Evaluating for symbol {symbol_id} ...\")\n",
    "        features, responders, target = create_timeseries_for_symbol(df, symbol_id)\n",
    "        X, y = prepare_regression_data(features, responders, target)\n",
    "        \n",
    "        # Train and evaluate model\n",
    "        model, X_train, X_test, y_train, y_test = train_best_xgboost_model(X, y)\n",
    "        evaluate_best_model(model, X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_prediction_data(features_df, lags_df):\n",
    "    \"\"\"\n",
    "    Prepare data for prediction by:\n",
    "    1. Getting clean features (no NaN)\n",
    "    2. Combining with lagged responders (excluding responder_6)\n",
    "    \"\"\"\n",
    "    # Get clean features\n",
    "    clean_features = features_df.loc[:, ~features_df.isna().any()]\n",
    "    feature_cols = [col for col in clean_features.columns if col.startswith('feature_')]\n",
    "    clean_features = clean_features[feature_cols]\n",
    "    \n",
    "    # Get lagged responders (excluding responder_6)\n",
    "    lag_cols = [col for col in lags_df.columns if col.startswith('responder_') and not col.startswith('responder_6')]\n",
    "    responder_lags = lags_df[lag_cols]\n",
    "    \n",
    "    # Combine features and responders\n",
    "    X = pd.concat([clean_features, responder_lags], axis=1)\n",
    "    \n",
    "    print(\"\\nPrediction data preparation:\")\n",
    "    print(f\"Number of clean features: {len(feature_cols)}\")\n",
    "    print(f\"Number of lagged responders: {len(lag_cols)}\")\n",
    "    print(f\"Final X shape: {X.shape}\")\n",
    "    \n",
    "    return X\n",
    "\n",
    "def make_predictions(model, features_df, lags_df):\n",
    "    \"\"\"\n",
    "    Use trained model to predict responder_6\n",
    "    \"\"\"\n",
    "    # Prepare prediction data\n",
    "    X = prepare_prediction_data(features_df, lags_df)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = model.predict(X)\n",
    "    \n",
    "    # Create output DataFrame\n",
    "    results = pd.DataFrame({\n",
    "        'symbol_id': features_df['symbol_id'],\n",
    "        'predicted_responder_6': predictions\n",
    "    })\n",
    "    \n",
    "    if 'responder_6_lag_1' in lags_df.columns:\n",
    "        results['actual_lag'] = lags_df['responder_6_lag_1']\n",
    "        \n",
    "    if 'weight' in features_df.columns:\n",
    "        results['weight'] = features_df['weight']\n",
    "    \n",
    "    print(\"\\nPrediction Results:\")\n",
    "    print(results)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load test data\n",
    "    features = pd.read_parquet(\"test.parquet/date_id=0/part-0.parquet\")\n",
    "    responders = pd.read_parquet(\"lags.parquet/date_id=0/part-0.parquet\")\n",
    "    \n",
    "    # Assuming we have a trained model\n",
    "    if 'model' in globals():  # Check if model exists\n",
    "        predictions = make_predictions(model, features, responders)\n",
    "    else:\n",
    "        print(\"\\nPlease ensure a trained model exists before running predictions\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
