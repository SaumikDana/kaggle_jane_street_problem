{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "\n",
    "def clean_data(features, responders):\n",
    "\n",
    "    # Print columns before cleaning\n",
    "    print(f\"\\nTotal feature columns before cleaning: {len(features.columns)}\")\n",
    "    print(f\"Total responder columns before cleaning: {len(responders.columns)}\")\n",
    "    \n",
    "    # Get clean features (no NaN)\n",
    "    clean_features = features.loc[:, ~features.isna().any()].reset_index(drop=True)\n",
    "    \n",
    "    # Get clean responders (no NaN)\n",
    "    clean_responders = responders.loc[:, ~responders.isna().any()].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\nNumber of clean features: {len(clean_features.columns)}\")\n",
    "    print(f\"Number of clean responders: {len(clean_responders.columns)}\")\n",
    "    \n",
    "    return clean_features, clean_responders\n",
    "\n",
    "\n",
    "def create_timeseries_for_symbol(df, symbol_id):\n",
    "\n",
    "    # Filter for our symbol, then sort by date_id and time_id \n",
    "    df_for_symbol = df[df['symbol_id'] == symbol_id].copy()\n",
    "    symbol_data = df_for_symbol.sort_values(['date_id', 'time_id'])\n",
    "    \n",
    "    # Get column names\n",
    "    feature_cols = [col for col in df.columns if col.startswith('feature_')]\n",
    "    responder_cols = [col for col in df.columns if col.startswith('responder_') and col != 'responder_6']\n",
    "    target_col = 'responder_6'\n",
    "    \n",
    "    # Get first date and its last time for responders\n",
    "    first_date = symbol_data['date_id'].min()\n",
    "    first_date_last_time = symbol_data[symbol_data['date_id'] == first_date]['time_id'].max()\n",
    "    first_date_last_responders = symbol_data[\n",
    "        (symbol_data['date_id'] == first_date) &\n",
    "        (symbol_data['time_id'] == first_date_last_time)\n",
    "    ][responder_cols]\n",
    "    \n",
    "    # Get all data after first date (for features)\n",
    "    feature_series = symbol_data[symbol_data['date_id'] > first_date][feature_cols].copy()\n",
    "    \n",
    "    # Get all data after first date (for target)\n",
    "    target_series = symbol_data[symbol_data['date_id'] > first_date][target_col].copy()\n",
    "    target_series = target_series.reset_index(drop=True)\n",
    "    \n",
    "    # Get all data after first date except the last row (for responders)\n",
    "    responder_data = symbol_data[symbol_data['date_id'] > first_date][responder_cols].iloc[:-1]\n",
    "    \n",
    "    # Add first date's last responders at the start\n",
    "    responder_series = pd.concat([first_date_last_responders, responder_data])\n",
    "    \n",
    "    # Print verification\n",
    "    print(f\"\\nFeature series shape: {feature_series.shape}\")\n",
    "    print(f\"\\nResponder series shape: {responder_series.shape}\")\n",
    "    print(f\"\\nTarget series shape: {target_series.shape}\")\n",
    "    \n",
    "    return feature_series, responder_series, target_series\n",
    "\n",
    "\n",
    "def prepare_regression_data(features, responders, target=None):\n",
    "\n",
    "    if target is None:\n",
    "        common_indices = features.index.intersection(responders.index)\n",
    "    else:\n",
    "        common_indices = features.index.intersection(responders.index).intersection(target.index)\n",
    "\n",
    "    X = pd.concat([features.loc[common_indices], responders.loc[common_indices]], axis=1)\n",
    "\n",
    "    if target is not None:\n",
    "        y = target.loc[common_indices]\n",
    "\n",
    "    if target is not None:\n",
    "        print(f\"\\nRegression data shapes:\")\n",
    "\n",
    "    print(f\"X shape: {X.shape}\")\n",
    "\n",
    "    if target is not None:\n",
    "        print(f\"y shape: {y.shape}\")\n",
    "\n",
    "    return X, y if target is not None else X\n",
    "\n",
    "\n",
    "def sample_training_data(X_train, y_train, sample_fraction=1/10, n_bins=10):\n",
    "\n",
    "    # Handle RF and XGB with binning\n",
    "    sample_size = int(len(X_train) * sample_fraction)\n",
    "\n",
    "    # Initialize sampling variables\n",
    "    sampling_succeeded = False\n",
    "    X_train_sampled = None\n",
    "    y_train_sampled = None\n",
    "\n",
    "    # Try different binning approaches\n",
    "    y_train_df = pd.DataFrame(y_train, columns=['target'])\n",
    "\n",
    "    try:\n",
    "        # Try quantile-based bins first\n",
    "        y_train_df['bins'] = pd.qcut(y_train_df['target'], q=n_bins, labels=False, duplicates='drop')\n",
    "        sampling_succeeded = True\n",
    "    except ValueError:\n",
    "        try:\n",
    "            # If quantile binning fails, try equal-width bins\n",
    "            y_train_df['bins'] = pd.cut(y_train_df['target'], bins=n_bins, labels=False)\n",
    "            sampling_succeeded = True\n",
    "        except ValueError:\n",
    "            print(\"Warning: Binning methods failed. Falling back to random sampling.\")\n",
    "            sampling_succeeded = False\n",
    "\n",
    "    if sampling_succeeded:\n",
    "        # Proceed with bin-based sampling\n",
    "        bin_sample_size = max(1, sample_size // n_bins)  # Ensure at least 1 sample per bin\n",
    "        sampled_indices = []\n",
    "        \n",
    "        # Get unique bins that actually exist in the data\n",
    "        existing_bins = y_train_df['bins'].dropna().unique()\n",
    "        \n",
    "        for bin_idx in existing_bins:\n",
    "            bin_indices = y_train_df[y_train_df['bins'] == bin_idx].index.values  # Get numpy array\n",
    "            if len(bin_indices) > 0:\n",
    "                n_samples = min(len(bin_indices), bin_sample_size)\n",
    "                sampled_bin_indices = np.random.choice(bin_indices, n_samples, replace=False)\n",
    "                sampled_indices.extend(sampled_bin_indices)\n",
    "        \n",
    "        # Convert to numpy array for indexing\n",
    "        sampled_indices = np.array(sampled_indices)\n",
    "        \n",
    "        # Verify we got some samples\n",
    "        if len(sampled_indices) > 0:\n",
    "            X_train_sampled = X_train[sampled_indices]\n",
    "            y_train_sampled = y_train[sampled_indices]\n",
    "        else:\n",
    "            sampling_succeeded = False\n",
    "\n",
    "    # If all sampling methods failed or got no samples, fall back to random sampling\n",
    "    if not sampling_succeeded or X_train_sampled is None or len(X_train_sampled) == 0:\n",
    "        print(\"Falling back to random sampling...\")\n",
    "        sampled_indices = np.random.choice(\n",
    "            len(X_train), \n",
    "            size=min(sample_size, len(X_train)), \n",
    "            replace=False\n",
    "        )\n",
    "        X_train_sampled = X_train[sampled_indices]\n",
    "        y_train_sampled = y_train[sampled_indices]\n",
    "\n",
    "    # Verify final samples\n",
    "    if len(X_train_sampled) == 0 or len(y_train_sampled) == 0:\n",
    "        raise ValueError(\"Failed to create valid samples for tree-based models\")\n",
    "\n",
    "    print(\"\\nSampled data shapes:\")\n",
    "    print(f\"X shape: {X_train_sampled.shape} (samples, features+responders)\")\n",
    "    print(f\"y shape: {y_train_sampled.shape}\")\n",
    "\n",
    "    return X_train_sampled, y_train_sampled\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "def reduce_dimensions_pca(X, n_components=None, variance_threshold=None):\n",
    "\n",
    "   scaler = StandardScaler()\n",
    "   X_scaled = scaler.fit_transform(X)\n",
    "   \n",
    "   if n_components is None and variance_threshold is None:\n",
    "       n_components = min(X.shape[1], 10)  # Default to 10 components\n",
    "       \n",
    "   if variance_threshold:\n",
    "       temp_pca = PCA()\n",
    "       temp_pca.fit(X_scaled)\n",
    "       cumsum = np.cumsum(temp_pca.explained_variance_ratio_)\n",
    "       n_components = np.argmax(cumsum >= variance_threshold) + 1\n",
    "       \n",
    "   pca = PCA(n_components=n_components)\n",
    "   X_reduced = pca.fit_transform(X_scaled)\n",
    "   \n",
    "   print(f\"\\nVariance explained: {np.sum(pca.explained_variance_ratio_):.3f}\")\n",
    "   \n",
    "   return X_reduced, pca, scaler\n",
    "\n",
    "\n",
    "def prepare_prediction_data(features_df, lags_df):\n",
    "\n",
    "    # Get clean features\n",
    "    clean_features = features_df.loc[:, ~features_df.isna().any()]\n",
    "    feature_cols = [col for col in clean_features.columns if col.startswith('feature_')]\n",
    "    clean_features = clean_features[feature_cols]\n",
    "    \n",
    "    # Get lagged responders (excluding responder_6)\n",
    "    lags_df.columns = [col.replace('_lag_1', '') if col.startswith('responder_') else col for col in lags_df.columns]\n",
    "    lag_cols = [col for col in lags_df.columns if col.startswith('responder_') and not col.startswith('responder_6')]\n",
    "    responder_lags = lags_df[lag_cols]\n",
    "    \n",
    "    # Combine features and responders\n",
    "    X = pd.concat([clean_features, responder_lags], axis=1)\n",
    "    \n",
    "    print(\"\\nPrediction data preparation:\")\n",
    "    print(f\"Number of clean features: {len(feature_cols)}\")\n",
    "    print(f\"Number of lagged responders: {len(lag_cols)}\")\n",
    "    print(f\"Final X shape: {X.shape}\")\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_partition = {}\n",
    "for partition in range(10):\n",
    "    partition_path = f\"train.parquet/partition_id={partition}/part-0.parquet\"\n",
    "    df_partition[partition] = pd.read_parquet(partition_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "n = 1\n",
    "\n",
    "# Initialize the model once\n",
    "model = XGBRegressor(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    min_child_weight=3,\n",
    "    subsample=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Initialize lists to store all training data\n",
    "all_X_reduced = []\n",
    "all_y = []\n",
    "pca = {}\n",
    "scaler = {}\n",
    "columns_to_keep = {}\n",
    "\n",
    "# First pass: collect and process all data\n",
    "for symbol_id in range(n):\n",
    "\n",
    "    print(f'\\nProcessing raw data for symbol {symbol_id} ...')\n",
    "\n",
    "    # Read and combine data for this symbol from all partitions\n",
    "    symbol_data = []\n",
    "    for partition in range(10):\n",
    "        df = df_partition[partition]\n",
    "        symbol_partition = df[df['symbol_id'] == symbol_id]\n",
    "        if not symbol_partition.empty:\n",
    "            symbol_data.append(symbol_partition)\n",
    "    \n",
    "    if symbol_data:\n",
    "        df_symbol = pd.concat(symbol_data, ignore_index=True)\n",
    "\n",
    "    print(f'\\nDone processing raw data for symbol {symbol_id} ...')\n",
    "\n",
    "    features, responders, target = create_timeseries_for_symbol(df_symbol, symbol_id)\n",
    "    clean_features, clean_responders = clean_data(features, responders)\n",
    "    X, y = prepare_regression_data(clean_features, clean_responders, target)\n",
    "    columns_to_keep[symbol_id] = X.columns\n",
    "    \n",
    "    # Reduce dimensions\n",
    "    X_reduced, pca[symbol_id], scaler[symbol_id] = reduce_dimensions_pca(X.values, n_components=25)\n",
    "\n",
    "    # Convert inputs to numpy arrays if they're pandas DataFrames\n",
    "    if isinstance(y, pd.Series):\n",
    "        y = y.to_numpy()\n",
    "\n",
    "    X_sampled, y_sampled = sample_training_data(X_reduced, y)\n",
    "\n",
    "    # Store processed data\n",
    "    all_X_reduced.append(X_sampled)\n",
    "    all_y.append(y_sampled)\n",
    "\n",
    "# Combine all data\n",
    "X_combined = np.vstack(all_X_reduced)\n",
    "y_combined = np.concatenate(all_y)\n",
    "\n",
    "# Single train-test split on combined data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y_combined, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model once on all data\n",
    "print(\"\\nTraining model on combined data...\")\n",
    "model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_test, y_test)], verbose=True)\n",
    "print(\"\\nCompleted Training model on combined data...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lags_ : pl.DataFrame | None = None\n",
    "\n",
    "def predict(test: pl.DataFrame, lags: pl.DataFrame | None) -> pl.DataFrame | pd.DataFrame:\n",
    "    \"\"\"Make a prediction.\"\"\"\n",
    "    global lags_\n",
    "    if lags is not None:\n",
    "        lags_ = lags\n",
    "\n",
    "    ##########################\n",
    "    predictions = pd.DataFrame(columns=['row_id', 'responder_6'])\n",
    "\n",
    "    # Make predictions for each symbol\n",
    "    for symbol_id in range(n):\n",
    "        symbol_features = test[test['symbol_id'] == symbol_id].copy()\n",
    "        row_id = symbol_features['row_id'].values[0]\n",
    "\n",
    "        symbol_responders = lags[lags['symbol_id'] == symbol_id].copy()\n",
    "\n",
    "        X_test = prepare_prediction_data(symbol_features, symbol_responders)\n",
    "        \n",
    "        # Use the same scaler and PCA from the last iteration\n",
    "        X_test_scaled = scaler[symbol_id].transform(X_test[columns_to_keep[symbol_id]].values)\n",
    "        X_test_reduced = pca[symbol_id].transform(X_test_scaled)\n",
    "        \n",
    "        # Make prediction\n",
    "        prediction = model.predict(X_test_reduced)\n",
    "        \n",
    "        # Create temporary DataFrame\n",
    "        temp_df = pd.DataFrame({'row_id': row_id, 'responder_6': prediction})\n",
    "        \n",
    "        # Append to main DataFrame\n",
    "        predictions = pd.concat([predictions, temp_df], ignore_index=True)\n",
    "\n",
    "    # Sort by row_id\n",
    "    predictions = predictions.sort_values('row_id').reset_index(drop=True)\n",
    "    ##########################\n",
    "    \n",
    "    if isinstance(predictions, pl.DataFrame):\n",
    "        assert predictions.columns == ['row_id', 'responder_6']\n",
    "    elif isinstance(predictions, pd.DataFrame):\n",
    "        assert (predictions.columns == ['row_id', 'responder_6']).all()\n",
    "    else:\n",
    "        raise TypeError('The predict function must return a DataFrame')\n",
    "    # Confirm has as many rows as the test data.\n",
    "    assert len(predictions) == len(test)\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "features_test = pd.read_parquet(\"test.parquet/date_id=0/part-0.parquet\")\n",
    "responders_test = pd.read_parquet(\"lags.parquet/date_id=0/part-0.parquet\")\n",
    "\n",
    "predict(features_test, responders_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
