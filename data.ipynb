{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def create_timeseries_for_symbol(df, symbol_id):\n",
    "    \"\"\"\n",
    "    Create feature and responder time series for a given symbol\n",
    "    \n",
    "    Args:\n",
    "        df: Input dataframe\n",
    "        symbol_id: Symbol to process\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (feature_series, responder_series)\n",
    "    \"\"\"\n",
    "    # Sort by date_id and time_id, then filter for our symbol\n",
    "    df_sorted = df.sort_values(['date_id', 'time_id'])\n",
    "    symbol_data = df_sorted[df_sorted['symbol_id'] == symbol_id].copy()\n",
    "    \n",
    "    # Get column names\n",
    "    feature_cols = [col for col in df.columns if col.startswith('feature_')]\n",
    "    responder_cols = [col for col in df.columns if col.startswith('responder_') and col != 'responder_6']\n",
    "\n",
    "    target_col = 'responder_6'\n",
    "    \n",
    "    # Get first date and its last time for responders\n",
    "    first_date = symbol_data['date_id'].min()\n",
    "    first_date_last_time = symbol_data[symbol_data['date_id'] == first_date]['time_id'].max()\n",
    "    first_date_last_responders = symbol_data[\n",
    "        (symbol_data['date_id'] == first_date) & \n",
    "        (symbol_data['time_id'] == first_date_last_time)\n",
    "    ][responder_cols]\n",
    "    \n",
    "    # Get all data after first date (for features)\n",
    "    feature_series = symbol_data[symbol_data['date_id'] > first_date][feature_cols].copy()\n",
    "\n",
    "    # Get all data after first date (for target)\n",
    "    target_series = symbol_data[symbol_data['date_id'] > first_date][target_col].copy()\n",
    "\n",
    "    # Get all data after first date except the last row (for responders)\n",
    "    responder_data = symbol_data[symbol_data['date_id'] > first_date][responder_cols].iloc[:-1]\n",
    "    \n",
    "    # Add first date's last responders at the start\n",
    "    responder_series = pd.concat([first_date_last_responders, responder_data])\n",
    "    \n",
    "    # Print verification\n",
    "    print(f\"\\nFeature series shape: {feature_series.shape}\")\n",
    "    print(\"\\nFirst few rows of feature series:\")\n",
    "    print(feature_series.head())\n",
    "\n",
    "    print(f\"\\nResponder series shape: {responder_series.shape}\")\n",
    "    print(\"\\nFirst few rows of responder series (should start with final time_id of first date):\")\n",
    "    print(responder_series.head())\n",
    "\n",
    "    print(f\"\\nTarget series shape: {target_series.shape}\")\n",
    "    print(\"\\nFirst few rows of target series:\")\n",
    "    print(target_series.head())\n",
    "\n",
    "    return feature_series, responder_series, target_series\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load data\n",
    "    df = pd.read_parquet(\"train.parquet/partition_id=0/part-0.parquet\")\n",
    "    symbol_id = df['symbol_id'].unique()[0]\n",
    "    \n",
    "    # Create time series\n",
    "    features, responders, target = create_timeseries_for_symbol(df, symbol_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "features_df = pd.read_csv('features.csv')\n",
    "\n",
    "# Look at first few features that show NaN in your time series\n",
    "nan_features = ['feature_00', 'feature_01', 'feature_02', 'feature_03', 'feature_04', 'feature_73', 'feature_74']\n",
    "\n",
    "# Extract tag patterns for these features\n",
    "nan_patterns = features_df[features_df['feature'].isin(nan_features)]\n",
    "print(\"Tag patterns for features that show NaN values:\")\n",
    "print(nan_patterns)\n",
    "\n",
    "# Count True values for each tag column in NaN features\n",
    "print(\"\\nCount of True values for each tag in NaN features:\")\n",
    "true_counts = nan_patterns.iloc[:, 1:].sum()\n",
    "print(true_counts[true_counts > 0])  # Only show tags that are True for any of these features\n",
    "\n",
    "# Also look at some features that don't have NaN\n",
    "non_nan_features = ['feature_05', 'feature_06', 'feature_07', 'feature_08', 'feature_09']\n",
    "non_nan_patterns = features_df[features_df['feature'].isin(non_nan_features)]\n",
    "print(\"\\nTag patterns for features that don't show NaN values:\")\n",
    "print(non_nan_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_separate_timeseries(features, responders, target):\n",
    "    \"\"\"\n",
    "    Create separate subplots for first 5 features and all 8 responders\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    clean_features_plotted = 0\n",
    "    all_features = features.columns\n",
    "    \n",
    "    # Keep plotting until we get 5 clean features\n",
    "    for feature_name in all_features:\n",
    "        # Check if this feature has any NaN\n",
    "        if not features[feature_name].isna().any():\n",
    "            ax = plt.subplot(1, 5, clean_features_plotted + 1)\n",
    "            timeseries = features[feature_name]\n",
    "            ax.plot(timeseries, color='blue', alpha=0.7)\n",
    "            ax.set_title(feature_name)\n",
    "            ax.grid(True)\n",
    "            if clean_features_plotted == 0:\n",
    "                ax.set_ylabel('Value')\n",
    "            ax.set_xlabel('Time Steps')\n",
    "            \n",
    "            clean_features_plotted += 1\n",
    "            if clean_features_plotted == 5:  # Stop after 5 clean features\n",
    "                break\n",
    "    \n",
    "    plt.suptitle('First 5 Non-NaN Features', y=1.05)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Second plot - Responders\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    for i, col in enumerate(responders.columns):\n",
    "        ax = plt.subplot(2, 4, i+1)  # 2 rows, 4 columns\n",
    "        ax.plot(responders[col], color='red', alpha=0.7)\n",
    "        ax.set_title(f'Responder {col[-1]}')\n",
    "        ax.grid(True)\n",
    "        if i % 4 == 0:  # Add y-label for leftmost plots\n",
    "            ax.set_ylabel('Value')\n",
    "        ax.set_xlabel('Time Steps')\n",
    "    plt.suptitle('All 8 Responders', y=1.05)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Third plot - Target\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(target, color='green', alpha=0.7)\n",
    "    plt.title('Target (responder_6)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load data\n",
    "    df = pd.read_parquet(\"train.parquet/partition_id=0/part-0.parquet\")\n",
    "    symbol_id = df['symbol_id'].unique()[0]\n",
    "    \n",
    "    # Create time series\n",
    "    features, responders, target = create_timeseries_for_symbol(df, symbol_id)\n",
    "    \n",
    "    # Plot\n",
    "    plot_separate_timeseries(features, responders, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def prepare_regression_data(features, responders, target):\n",
    "    \"\"\"\n",
    "    Prepare X and y for regression by:\n",
    "    1. Removing features with NaN\n",
    "    2. Combining clean features with responders\n",
    "    3. Aligning with target\n",
    "    \"\"\"\n",
    "    # Get clean features (no NaN)\n",
    "    clean_features = features.loc[:, ~features.isna().any()].reset_index(drop=True)\n",
    "    responders = responders.reset_index(drop=True)\n",
    "    target = target.reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Number of clean features: {len(clean_features.columns)}\")\n",
    "    \n",
    "    # Make sure all have same length\n",
    "    min_len = min(len(clean_features), len(responders), len(target))\n",
    "    clean_features = clean_features.iloc[:min_len]\n",
    "    responders = responders.iloc[:min_len]\n",
    "    target = target.iloc[:min_len]\n",
    "    \n",
    "    # Combine clean features and responders for X\n",
    "    X = pd.concat([clean_features, responders], axis=1)\n",
    "    y = target\n",
    "    \n",
    "    print(\"\\nRegression data shapes:\")\n",
    "    print(f\"X shape: {X.shape} (samples, features+responders)\")\n",
    "    print(f\"y shape: {y.shape}\")\n",
    "    \n",
    "    # Print first few column names to verify\n",
    "    print(\"\\nFirst few X columns:\")\n",
    "    print(\"Features:\", clean_features.columns[:5].tolist())\n",
    "    print(\"Responders:\", responders.columns[:5].tolist())\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def train_model(X, y):\n",
    "    \"\"\"\n",
    "    Train a linear regression model\n",
    "    \"\"\"\n",
    "    # Split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    print(\"Train/Test split sizes:\")\n",
    "    print(f\"X_train: {X_train.shape}\")\n",
    "    print(f\"X_test: {X_test.shape}\")\n",
    "    \n",
    "    # Train model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    return model, X_train, X_test, y_train, y_test\n",
    "\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Evaluate its performance\n",
    "    \"\"\"\n",
    "    # Make predictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "    test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    \n",
    "    print(\"\\nModel Performance:\")\n",
    "    print(f\"Train MSE: {train_mse:.4f}\")\n",
    "    print(f\"Test MSE: {test_mse:.4f}\")\n",
    "    print(f\"Train R²: {train_r2:.4f}\")\n",
    "    print(f\"Test R²: {test_r2:.4f}\")\n",
    "    \n",
    "    # Plot actual vs predicted for test set\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    # First subplot: Actual vs Predicted scatter\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(y_test, y_test_pred, alpha=0.5)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "    plt.xlabel('Actual')\n",
    "    plt.ylabel('Predicted')\n",
    "    plt.title('Actual vs Predicted (Test Set)')\n",
    "    \n",
    "    # Second subplot: Residuals\n",
    "    plt.subplot(1, 2, 2)\n",
    "    residuals = y_test - y_test_pred\n",
    "    plt.hist(residuals, bins=50)\n",
    "    plt.xlabel('Residual')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Residuals Distribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load data\n",
    "    df = pd.read_parquet(\"train.parquet/partition_id=0/part-0.parquet\")\n",
    "    symbol_id = df['symbol_id'].unique()[0]\n",
    "    \n",
    "    # Create time series\n",
    "    features, responders, target = create_timeseries_for_symbol(df, symbol_id)\n",
    "    \n",
    "    # Prepare regression data\n",
    "    X, y = prepare_regression_data(features, responders, target)\n",
    "    \n",
    "    # Train and evaluate model\n",
    "    model, X_train, X_test, y_train, y_test = train_model(X, y)\n",
    "    evaluate_model(model, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "def prepare_regression_data_responders_only(features, responders, target):\n",
    "    \"\"\"\n",
    "    Prepare X and y for regression using only responders\n",
    "    \"\"\"\n",
    "    # Reset indices\n",
    "    responders = responders.reset_index(drop=True)\n",
    "    target = target.reset_index(drop=True)\n",
    "    \n",
    "    # Make sure all have same length\n",
    "    min_len = min(len(responders), len(target))\n",
    "    responders = responders.iloc[:min_len]\n",
    "    target = target.iloc[:min_len]\n",
    "    \n",
    "    # X is just responders\n",
    "    X = responders\n",
    "    y = target\n",
    "    \n",
    "    print(\"\\nRegression data shapes:\")\n",
    "    print(f\"X shape: {X.shape} (samples, responders)\")\n",
    "    print(f\"y shape: {y.shape}\")\n",
    "    print(\"\\nResponder columns:\")\n",
    "    print(\"Responders:\", X.columns.tolist())\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def prepare_regression_data_top3_responders(features, responders, target):\n",
    "    \"\"\"\n",
    "    Prepare X and y for regression using only top 3 responders (3, 0, and 7)\n",
    "    \"\"\"\n",
    "    # Reset indices\n",
    "    responders = responders.reset_index(drop=True)\n",
    "    target = target.reset_index(drop=True)\n",
    "    \n",
    "    # Make sure all have same length\n",
    "    min_len = min(len(responders), len(target))\n",
    "    responders = responders.iloc[:min_len]\n",
    "    target = target.iloc[:min_len]\n",
    "    \n",
    "    # Select only top 3 responders\n",
    "    top_responders = ['responder_3', 'responder_0', 'responder_7']\n",
    "    X = responders[top_responders]\n",
    "    y = target\n",
    "    \n",
    "    print(\"\\nRegression data shapes:\")\n",
    "    print(f\"X shape: {X.shape} (samples, top 3 responders)\")\n",
    "    print(f\"y shape: {y.shape}\")\n",
    "    print(\"\\nTop 3 responder columns:\")\n",
    "    print(\"Responders:\", X.columns.tolist())\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def train_xgboost_model(X, y):\n",
    "    \"\"\"\n",
    "    Train an XGBoost regression model with basic hyperparameters\n",
    "    \"\"\"\n",
    "    # Split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    print(\"Train/Test split sizes:\")\n",
    "    print(f\"X_train: {X_train.shape}\")\n",
    "    print(f\"X_test: {X_test.shape}\")\n",
    "    \n",
    "    # Initialize and train model\n",
    "    model = XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=6,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train the model with eval set for validation\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    return model, X_train, X_test, y_train, y_test\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load data\n",
    "    df = pd.read_parquet(\"train.parquet/partition_id=0/part-0.parquet\")\n",
    "    symbol_id = df['symbol_id'].unique()[0]\n",
    "    \n",
    "    # Create time series\n",
    "    features, responders, target = create_timeseries_for_symbol(df, symbol_id)\n",
    "    \n",
    "    # Prepare regression data\n",
    "    X, y = prepare_regression_data(features, responders, target)\n",
    "    \n",
    "    # Train and evaluate model\n",
    "    model, X_train, X_test, y_train, y_test = train_xgboost_model(X, y)\n",
    "    evaluate_model(model, X_train, X_test, y_train, y_test)\n",
    "\n",
    "    # Print feature importances\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 most important features:\")\n",
    "    print(feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_and_evaluate_multiple_models(X, y):\n",
    "    \"\"\"\n",
    "    Train and evaluate multiple models on the same data\n",
    "    \"\"\"\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Define models\n",
    "    models = {\n",
    "        'Linear Regression': LinearRegression(),\n",
    "        'Ridge': Ridge(alpha=1.0),\n",
    "        'Lasso': Lasso(alpha=1.0),\n",
    "        'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "        'XGBoost': XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=6, random_state=42)\n",
    "    }\n",
    "    \n",
    "    # Train and evaluate each model\n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nTraining {name}...\")\n",
    "        \n",
    "        # Train\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "        test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "        train_r2 = r2_score(y_train, y_train_pred)\n",
    "        test_r2 = r2_score(y_test, y_test_pred)\n",
    "        \n",
    "        print(f\"{name} Performance:\")\n",
    "        print(f\"Train MSE: {train_mse:.4f}\")\n",
    "        print(f\"Test MSE: {test_mse:.4f}\")\n",
    "        print(f\"Train R²: {train_r2:.4f}\")\n",
    "        print(f\"Test R²: {test_r2:.4f}\")\n",
    "        \n",
    "        # Store results\n",
    "        results[name] = {\n",
    "            'model': model,\n",
    "            'test_pred': y_test_pred,\n",
    "            'test_actual': y_test,\n",
    "            'metrics': {\n",
    "                'train_mse': train_mse,\n",
    "                'test_mse': test_mse,\n",
    "                'train_r2': train_r2,\n",
    "                'test_r2': test_r2\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # Plot comparison of actual vs predicted for all models\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i, (name, result) in enumerate(results.items(), 1):\n",
    "        plt.subplot(2, 3, i)\n",
    "        plt.scatter(result['test_actual'], result['test_pred'], alpha=0.5)\n",
    "        plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "        plt.xlabel('Actual')\n",
    "        plt.ylabel('Predicted')\n",
    "        plt.title(f'{name}\\nTest R²: {result[\"metrics\"][\"test_r2\"]:.4f}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and prepare data\n",
    "    df = pd.read_parquet(\"train.parquet/partition_id=0/part-0.parquet\")\n",
    "    symbol_id = df['symbol_id'].unique()[0]\n",
    "    features, responders, target = create_timeseries_for_symbol(df, symbol_id)\n",
    "    X, y = prepare_regression_data_top3_responders(features, responders, target)\n",
    "    \n",
    "    # Train and evaluate multiple models\n",
    "    results = train_and_evaluate_multiple_models(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "def tune_xgboost(X, y):\n",
    "    \"\"\"\n",
    "    Tune XGBoost hyperparameters using GridSearchCV\n",
    "    \"\"\"\n",
    "    # Define parameter grid\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 4, 5, 6, 7],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'subsample': [0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "        'min_child_weight': [1, 3, 5]\n",
    "    }\n",
    "    \n",
    "    # Initialize XGBoost model\n",
    "    xgb = XGBRegressor(random_state=42)\n",
    "    \n",
    "    # Set up GridSearchCV\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=xgb,\n",
    "        param_grid=param_grid,\n",
    "        scoring='r2',\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "        verbose=2\n",
    "    )\n",
    "    \n",
    "    # Fit GridSearchCV\n",
    "    grid_search.fit(X, y)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nBest parameters found:\")\n",
    "    print(grid_search.best_params_)\n",
    "    print(f\"\\nBest cross-validation R²: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and prepare data (with all features and responders)\n",
    "    df = pd.read_parquet(\"train.parquet/partition_id=0/part-0.parquet\")\n",
    "    symbol_id = df['symbol_id'].unique()[0]\n",
    "    features, responders, target = create_timeseries_for_symbol(df, symbol_id)\n",
    "    X, y = prepare_regression_data(features, responders, target)\n",
    "    \n",
    "    # Tune model\n",
    "    best_model = tune_xgboost(X, y)\n",
    "    \n",
    "    # Evaluate best model\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    evaluate_model(best_model, X_train, X_test, y_train, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
